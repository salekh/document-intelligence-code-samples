{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Scale Document Analysis and Processing\n",
    "\n",
    "This notebook demonstrates an example of using [LangChain](https://www.langchain.com/) to delvelop a Retrieval Augmented Generation (RAG) pattern. It uses Azure AI Document Intelligence as document loader, which can extracts tables, paragraphs, and layout information from pdf, image, office and html files. The output markdown can be used in LangChain's markdown header splitter, which enables semantic chunking of the documents. Then the chunked documents are indexed into Azure AI Search vectore store. Given a user query, it will use Azure AI Search to get the relevant chunks, then feed the context into the prompt with the query to generate the answer.\n",
    "\n",
    "\n",
    "![Semantic chunking in RAG](https://github.com/microsoft/Form-Recognizer-Toolkit/blob/main/SampleCode/media/semantic-chunking-rag.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0) to create one if you don't have.\n",
    "- An Azure AI Search resource - follow [this document](https://learn.microsoft.com/azure/search/search-create-service-portal) to create one if you don't have.\n",
    "- An Azure OpenAI resource and deployments for embeddings model and chat model - follow [this document](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal) to create one if you don't have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-dotenv langchain langchain-core langchain-community langchain-openai langchainhub openai tiktoken azure-ai-documentintelligence azure-identity azure-search-documents==11.6.0b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code loads environment variables using the `dotenv` library and sets the necessary environment variables for Azure services.\n",
    "The environment variables are loaded from the `.env` file in the same directory as this notebook.\n",
    "\"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"OPENAI_API_VERSION\") or \"2024-05-01-preview\"\n",
    "doc_intelligence_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a document and split it into semantic chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Azure AI Document Intelligence to load the document. You can either specify file_path or url_path to load the document.\n",
    "project2_sas_token = r\"....\" #Your SAS token here\n",
    "loader = AzureAIDocumentIntelligenceLoader(url_path=project2_sas_token, api_key = doc_intelligence_key, api_endpoint = doc_intelligence_endpoint, api_model=\"prebuilt-layout\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the document into chunks base on markdown headers.\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs_string = docs[0].page_content\n",
    "splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Semantically Split Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([vars(split) for split in splits])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and index the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing the chunks is a great opportunity to:\n",
    "- Extract additional metadata from the chunks for better deterministic search\n",
    "- Embed the chunks for semantic search and hybrid search\n",
    "- Design a schema in way that allows for fast retrieval using filters\n",
    "\n",
    "Small Language Models are great at Document Structure Analysis. For this purpose, a local copy of a SLM could serve as a more cost-effective alternative for scale scenarios, while preserving quality.\n",
    "\n",
    "The first metadata item that should be added here are\n",
    "- Project ID\n",
    "- Project Name\n",
    "- Product ID\n",
    "- Product Name\n",
    "\n",
    "You will see that using Semantic Chunking via Markdown automatically also provides a hierarchy of the document structure. This is great for context expansion and establishing a parent-child relationship between the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['project_id'] = '2'\n",
    "df['project_name'] = 'Product A'\n",
    "df['product_id'] = '1'\n",
    "df['product_name'] = 'Cooler for Product A'\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's do the same thing for Document 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Azure AI Document Intelligence to load the document. You can either specify file_path or url_path to load the document.\n",
    "project1_sas_token = r\"....\" #Your SAS token here\n",
    "loader = AzureAIDocumentIntelligenceLoader(url_path=project1_sas_token, api_key = doc_intelligence_key, api_endpoint = doc_intelligence_endpoint, api_model=\"prebuilt-layout\")\n",
    "docs = loader.load()\n",
    "docs_string = docs[0].page_content\n",
    "splits_1 = text_splitter.split_text(docs_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([vars(split) for split in splits_1])\n",
    "df1['project_id'] = '1'\n",
    "df1['project_name'] = 'Product B'\n",
    "df1['product_id'] = '1'\n",
    "df1['product_name'] = 'Rotary Kiln for drying Lithium Hydroxide'\n",
    "df_combined = pd.concat([df, df1], ignore_index=True)\n",
    "df_combined = df_combined.drop(columns=['type'])\n",
    "df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the splitted documents and insert into Azure Search vector store\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"....\",  # e.g., \"text-embedding-3-large\"\n",
    "    openai_api_version=\"...\",  # e.g., \"2023-12-01-preview\"\n",
    ")\n",
    "\n",
    "vector_store_address: str = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "vector_store_password: str = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "index_name: str = \"...\" # name of your search index\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=aoai_embeddings.embed_query,\n",
    ")\n",
    "\n",
    "df_combined[\"metadata\"] = df_combined[\"metadata\"].apply(json.dumps)\n",
    "df_combined['id'] = [str(uuid.uuid4()) for _ in range(len(df_combined))]\n",
    "df_combined[\"page_content_vector\"] = df_combined[\"page_content\"].apply(aoai_embeddings.embed_query)\n",
    "df_combined_dict = df_combined.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now store the chunked and embedded documents into the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "search_client: SearchClient = SearchClient(endpoint=vector_store_address, index_name=index_name, credential=AzureKeyCredential(vector_store_password))\n",
    "search_client.upload_documents(documents=df_combined_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrive relevant chunks based on a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "# Retrieve relevant chunks based on the question\n",
    "query = \"equipment warranty conditions for project 1 rotary kiln\"\n",
    "query_embedding = aoai_embeddings.embed_query(query)\n",
    "vector_query = VectorizedQuery(vector=query_embedding, k_nearest_neighbors=3, fields=\"page_content_vector\")\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"page_content\", \"project_id\", \"project_name\", \"product_id\", \"product_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPT-4o to create optimized JSON Queries for Azure AI Search\n",
    "LLMs are great code generators, why not use this strength of theirs!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    api_version=\"...\", # e.g., \"2024-05-01-preview\"\n",
    "    azure_deployment=\"...\", # e.g., \"gpt-4o\"\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Let's try to check if the LLM can create filters for us\n",
    "index_definition = r\"\"\"{\n",
    "  \"@odata.context\": \"https://sa-cognitivesearch-1.search.windows.net/$metadata#indexes/$entity\",\n",
    "  \"@odata.etag\": \"\\\"0x8DC90792DAD43E3\\\"\",\n",
    "  \"name\": \"ge-lcp-docindex\",\n",
    "  \"defaultScoringProfile\": null,\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"name\": \"id\",\n",
    "      \"type\": \"Edm.String\",\n",
    "      \"searchable\": false,\n",
    "      \"filterable\": false,\n",
    "      \"retrievable\": true,\n",
    "      \"stored\": true,\n",
    "      \"sortable\": false,\n",
    "      \"facetable\": false,\n",
    "      \"key\": true,\n",
    "      \"indexAnalyzer\": null,\n",
    "      \"searchAnalyzer\": null,\n",
    "      \"analyzer\": null,\n",
    "      \"normalizer\": null,\n",
    "      \"dimensions\": null,\n",
    "      \"vectorSearchProfile\": null,\n",
    "      \"vectorEncoding\": null,\n",
    "      \"synonymMaps\": []\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"page_content\",\n",
    "      \"type\": \"Edm.String\",\n",
    "      \"searchable\": true,\n",
    "      \"filterable\": false,\n",
    "      \"retrievable\": true,\n",
    "      \"stored\": true,\n",
    "      \"sortable\": false,\n",
    "      \"facetable\": false,\n",
    "      \"key\": false,\n",
    "      \"indexAnalyzer\": null,\n",
    "      \"searchAnalyzer\": null,\n",
    "      \"analyzer\": \"standard.lucene\",\n",
    "      \"normalizer\": null,\n",
    "      \"dimensions\": null,\n",
    "      \"vectorSearchProfile\": null,\n",
    "      \"vectorEncoding\": null,\n",
    "      \"synonymMaps\": []\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"project_id\",\n",
    "      \"type\": \"Edm.String\",\n",
    "      \"searchable\": true,\n",
    "      \"filterable\": true,\n",
    "      \"retrievable\": true,\n",
    "      \"stored\": true,\n",
    "      \"sortable\": false,\n",
    "      \"facetable\": false,\n",
    "      \"key\": false,\n",
    "      \"indexAnalyzer\": null,\n",
    "      \"searchAnalyzer\": null,\n",
    "      \"analyzer\": \"standard.lucene\",\n",
    "      \"normalizer\": null,\n",
    "      \"dimensions\": null,\n",
    "      \"vectorSearchProfile\": null,\n",
    "      \"vectorEncoding\": null,\n",
    "      \"synonymMaps\": []\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"project_name\",\n",
    "      \"type\": \"Edm.String\",\n",
    "      \"searchable\": true,\n",
    "      \"filterable\": true,\n",
    "      \"retrievable\": true,\n",
    "      \"stored\": true,\n",
    "      \"sortable\": false,\n",
    "      \"facetable\": false,\n",
    "      \"key\": false,\n",
    "      \"indexAnalyzer\": null,\n",
    "      \"searchAnalyzer\": null,\n",
    "      \"analyzer\": \"standard.lucene\",\n",
    "      \"normalizer\": null,\n",
    "      \"dimensions\": null,\n",
    "      \"vectorSearchProfile\": null,\n",
    "      \"vectorEncoding\": null,\n",
    "      \"synonymMaps\": []\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"product_id\",\n",
    "      \"type\": \"Edm.String\",\n",
    "      \"searchable\": true,\n",
    "      \"filterable\": true,\n",
    "      \"retrievable\": true,\n",
    "      \"stored\": true,\n",
    "      \"sortable\": false,\n",
    "      \"facetable\": false,\n",
    "      \"key\": false,\n",
    "      \"indexAnalyzer\": null,\n",
    "      \"searchAnalyzer\": null,\n",
    "      \"analyzer\": \"standard.lucene\",\n",
    "      \"normalizer\": null,\n",
    "      \"dimensions\": null,\n",
    "      \"vectorSearchProfile\": null,\n",
    "      \"vectorEncoding\": null,\n",
    "      \"synonymMaps\": []\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"product_name\",\n",
    "      \"type\": \"Edm.String\",\n",
    "      \"searchable\": true,\n",
    "      \"filterable\": true,\n",
    "      \"retrievable\": true,\n",
    "      \"stored\": true,\n",
    "      \"sortable\": false,\n",
    "      \"facetable\": false,\n",
    "      \"key\": false,\n",
    "      \"indexAnalyzer\": null,\n",
    "      \"searchAnalyzer\": null,\n",
    "      \"analyzer\": \"standard.lucene\",\n",
    "      \"normalizer\": null,\n",
    "      \"dimensions\": null,\n",
    "      \"vectorSearchProfile\": null,\n",
    "      \"vectorEncoding\": null,\n",
    "      \"synonymMaps\": []\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"metadata\",\n",
    "      \"type\": \"Edm.String\",\n",
    "      \"searchable\": true,\n",
    "      \"filterable\": false,\n",
    "      \"retrievable\": true,\n",
    "      \"stored\": true,\n",
    "      \"sortable\": false,\n",
    "      \"facetable\": false,\n",
    "      \"key\": false,\n",
    "      \"indexAnalyzer\": null,\n",
    "      \"searchAnalyzer\": null,\n",
    "      \"analyzer\": \"standard.lucene\",\n",
    "      \"normalizer\": null,\n",
    "      \"dimensions\": null,\n",
    "      \"vectorSearchProfile\": null,\n",
    "      \"vectorEncoding\": null,\n",
    "      \"synonymMaps\": []\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"page_content_vector\",\n",
    "      \"type\": \"Collection(Edm.Single)\",\n",
    "      \"searchable\": true,\n",
    "      \"filterable\": false,\n",
    "      \"retrievable\": true,\n",
    "      \"stored\": true,\n",
    "      \"sortable\": false,\n",
    "      \"facetable\": false,\n",
    "      \"key\": false,\n",
    "      \"indexAnalyzer\": null,\n",
    "      \"searchAnalyzer\": null,\n",
    "      \"analyzer\": null,\n",
    "      \"normalizer\": null,\n",
    "      \"dimensions\": 3072,\n",
    "      \"vectorSearchProfile\": \"vector-profile-1718812341384\",\n",
    "      \"vectorEncoding\": null,\n",
    "      \"synonymMaps\": []\n",
    "    }\n",
    "  ],\n",
    "  \"scoringProfiles\": [],\n",
    "  \"corsOptions\": null,\n",
    "  \"suggesters\": [],\n",
    "  \"analyzers\": [],\n",
    "  \"normalizers\": [],\n",
    "  \"tokenizers\": [],\n",
    "  \"tokenFilters\": [],\n",
    "  \"charFilters\": [],\n",
    "  \"encryptionKey\": null,\n",
    "  \"similarity\": {\n",
    "    \"@odata.type\": \"#Microsoft.Azure.Search.BM25Similarity\",\n",
    "    \"k1\": null,\n",
    "    \"b\": null\n",
    "  },\n",
    "  \"semantic\": {\n",
    "    \"defaultConfiguration\": null,\n",
    "    \"configurations\": [\n",
    "      {\n",
    "        \"name\": \"semantic-ge-lcp-docindex\",\n",
    "        \"prioritizedFields\": {\n",
    "          \"titleField\": {\n",
    "            \"fieldName\": \"product_name\"\n",
    "          },\n",
    "          \"prioritizedContentFields\": [\n",
    "            {\n",
    "              \"fieldName\": \"page_content\"\n",
    "            },\n",
    "            {\n",
    "              \"fieldName\": \"metadata\"\n",
    "            }\n",
    "          ],\n",
    "          \"prioritizedKeywordsFields\": [\n",
    "            {\n",
    "              \"fieldName\": \"project_name\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"vectorSearch\": {\n",
    "    \"algorithms\": [\n",
    "      {\n",
    "        \"name\": \"vector-config-1718812342680\",\n",
    "        \"kind\": \"hnsw\",\n",
    "        \"hnswParameters\": {\n",
    "          \"metric\": \"cosine\",\n",
    "          \"m\": 4,\n",
    "          \"efConstruction\": 400,\n",
    "          \"efSearch\": 500\n",
    "        },\n",
    "        \"exhaustiveKnnParameters\": null\n",
    "      }\n",
    "    ],\n",
    "    \"profiles\": [\n",
    "      {\n",
    "        \"name\": \"vector-profile-1718812341384\",\n",
    "        \"algorithm\": \"vector-config-1718812342680\",\n",
    "        \"vectorizer\": \"vectorizer-1718812345799\",\n",
    "        \"compression\": \"vector-1718812082632-compressor\"\n",
    "      }\n",
    "    ],\n",
    "    \"vectorizers\": [\n",
    "      {\n",
    "        \"name\": \"vectorizer-1718812345799\",\n",
    "        \"kind\": \"azureOpenAI\",\n",
    "        \"azureOpenAIParameters\": {\n",
    "          \"resourceUri\": \"https://salekh-openai-swedenc.openai.azure.com\",\n",
    "          \"deploymentId\": \"salekh-swedenc-text-embedding-3-large\",\n",
    "          \"apiKey\": \"<redacted>\",\n",
    "          \"modelName\": \"text-embedding-3-large\",\n",
    "          \"authIdentity\": null\n",
    "        },\n",
    "        \"customWebApiParameters\": null,\n",
    "        \"aiServicesVisionParameters\": null,\n",
    "        \"amlParameters\": null\n",
    "      }\n",
    "    ],\n",
    "    \"compressions\": [\n",
    "      {\n",
    "        \"name\": \"vector-1718812082632-compressor\",\n",
    "        \"kind\": \"scalarQuantization\",\n",
    "        \"rerankWithOriginalVectors\": true,\n",
    "        \"defaultOversampling\": 4,\n",
    "        \"scalarQuantizationParameters\": {\n",
    "          \"quantizedDataType\": \"int8\"\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\"\"\"\n",
    "example_document = r\"\"\"{\n",
    "      \"id\": \"dcaf9c27-3e65-4022-8166-addd9e4a20c6\",\n",
    "      \"page_content\": \"||||\\n| - | - | - |\\n| :selected: X | Attachment | 1 Plot plan Product A Cooler System and Maintenance Arrangement |\\n| :selected: X | Attachment | 2 P&ID Product A Cooler Package |\\n| :selected: X | Attachment | 3 Sample Structure Sequences |\\n| :selected: X | Attachment | 4 Sample list of electrical consumers |\\n| :selected: X | Attachment | 5 Sample Mechanical Equipment List |\\n| :selected: X | Attachment | 6 BASF Technical Rule E-P-MC 911, Pressure Vessels Fabricated from Metallic Materials; Technical Standards to be met by the Manufacturer (Issue Oct 2021) |\\n| X :selected: :unselected: | Attachment | 6A BASF Technical Rule E-P-MC 911, Annex 8.2 Submission of Certification Documents in Electronic Form; Creation of Documents for Digital Certification (Issue Oct 2021) |\\n| :selected: X | Attachment | 6B BASF Technical Rule E-P-MC 911, Sample Form 100 EN (replaced by Annex 8.3)G-R-MC 100 M - Minimum Safety and Health Requirements - Machinery |  \\n<!-- PageFooter=\\\"CONFIDENTIAL\\\" -->\",\n",
    "      \"project_id\": \"2\",\n",
    "      \"project_name\": \"Product A\",\n",
    "      \"product_id\": \"1\",\n",
    "      \"product_name\": \"Cooler for Product A\",\n",
    "      \"metadata\": \"\\\"\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"Header 1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"Table of contents\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"Header 2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"11 Attachments\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\"\\\"\"\n",
    "    }\"\"\"\n",
    "prompt = f\"\"\"You are an AI chatbot that specializes in creating expert JSON queries for Azure AI Search. Create an optimized query based\n",
    "           on the following index definition and the example document. The query should retrieve the most relevant documents based on the\n",
    "           given example document. The query should be optimized for the given index definition. The query should be in JSON format. Try to\n",
    "           use filters whenever possible.\n",
    "           \n",
    "           Only provide a JSON answer. No other text output should be included. When I parse the incoming answer from the AI using json.loads,\n",
    "           I should get a valid Python dictionary. No exceptions are permitted.\n",
    "           Index Definition: {index_definition} \\n Example Document: {example_document} \\n Text Query: {query}\"\"\"\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "message = HumanMessage(content=prompt)\n",
    "print(llm.invoke([message]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a prompt for RAG that is checked into the LangChain prompt hub (https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=989ad331-949f-4bac-9694-660074a208a7)\n",
    "context = \"\\n\\n\".join(result[\"page_content\"] for result in results)\n",
    "\n",
    "rag_prompt = f\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "                Answer in a detailed, factually correct and intelligent manner.\\n\n",
    "                Question: {query} \\n\n",
    "                Context: {context} \\n\n",
    "                Answer:\"\"\"\n",
    "rag_message = HumanMessage(content=rag_prompt)\n",
    "print(llm.invoke([rag_message]).content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
